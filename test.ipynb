{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def stdwrite(buff):\n",
    "    sys.stdout.write('\\r' + buff)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from importlib import reload\n",
    "from simulation.simulation_builder import graph_builder as gb\n",
    "from simulation.architectures.mnist_architectures import cnn_mnist_architecture\n",
    "from tensorflow.python.client import device_lib\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "reload(gb)\n",
    "g = gb.GraphBuilder(cnn_mnist_architecture,\n",
    "                   learning_rate=0.01,\n",
    "                   noise_list=[1.0, 0.8, 0.6, 0.4],\n",
    "                   noise_type='dropout',\n",
    "                   name='cnn_mnist',\n",
    "                   summary_type='ordered_summary')\n",
    "#device_lib.list_local_devices()\n",
    "\n",
    "def stdwrite(buff):\n",
    "    sys.stdout.write('\\r' + buff)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "n_epochs = 25\n",
    "with g._graph.as_default():\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    data_path = 'simulation/data/mnist/'\n",
    "    mnist = input_data.read_data_sets(data_path)\n",
    "    test_feed_dict = g.create_feed_dict(mnist.test.images,\n",
    "                                       mnist.test.labels,\n",
    "                                       dataset_type='test')\n",
    "    valid_feed_dict = g.create_feed_dict(mnist.validation.images,\n",
    "                                        mnist.validation.labels,\n",
    "                                        dataset_type='validation')\n",
    "    step = 0\n",
    "    n_iters = n_iters = mnist.train.num_examples // batch_size\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(g.variable_initializer)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            evaluated = sess.run(g.get_train_ops(),\n",
    "                                feed_dict=valid_feed_dict)\n",
    "            g.add_summary(evaluated, step, dataset_type='validation')\n",
    "            g.update_noise_vals(evaluated)\n",
    "            for it in range(n_iters):\n",
    "                step += 1\n",
    "                X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "\n",
    "                evaluated = sess.run(g.get_train_ops(),\n",
    "                                    feed_dict=g.create_feed_dict(X_batch,\n",
    "                                                                y_batch))\n",
    "                g.add_summary(evaluated=evaluated,\n",
    "                             step=step)\n",
    "                if it % 200 == 0:\n",
    "                    evaluated = sess.run(g.get_train_ops('test'),\n",
    "                                        feed_dict=test_feed_dict)\n",
    "                    loss = g.extract_evaluated_tensors(evaluated, 'loss')\n",
    "                    g.add_summary(evaluated=evaluated,\n",
    "                             step=step, dataset_type='test')\n",
    "                    \n",
    "                    buff = ''\n",
    "                    for l in loss:\n",
    "                        buff = buff + ' ,' + str(l)\n",
    "                    stdwrite(buff)\n",
    "            \n",
    "                    #g.update_noise_vals(evaluated)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from simulation.simulation_builder import graph_builder as gb\n",
    "from importlib import reload\n",
    "from simulation.architectures.mnist_architectures import nn_mnist_architecture\n",
    "reload(gb)\n",
    "g = gb.GraphBuilder(nn_mnist_architecture,\n",
    "                   learning_rate=0.01,\n",
    "                   noise_list=[0.001, 0.003, 0.004, 0.009],\n",
    "                   noise_type='random_normal',\n",
    "                   name='nn_mnist',\n",
    "                   summary_type='ordered_summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "n_epochs = 25\n",
    "with g._graph.as_default():\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    data_path = 'simulation/data/mnist/'\n",
    "    mnist = input_data.read_data_sets(data_path)\n",
    "    test_feed_dict = g.create_feed_dict(mnist.test.images,\n",
    "                                       mnist.test.labels,\n",
    "                                       dataset_type='test')\n",
    "    valid_feed_dict = g.create_feed_dict(mnist.validation.images,\n",
    "                                        mnist.validation.labels,\n",
    "                                        dataset_type='validation')\n",
    "    step = 0\n",
    "    n_iters = n_iters = mnist.train.num_examples // batch_size\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(g.variable_initializer)\n",
    "        for epoch in range(n_epochs):\n",
    "            for it in range(n_iters):\n",
    "                step += 1\n",
    "                X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "\n",
    "                evaluated = sess.run(g.get_train_ops(),\n",
    "                                    feed_dict=g.create_feed_dict(X_batch,\n",
    "                                                                y_batch))\n",
    "                g.add_summary(evaluated, step)\n",
    "                if it % 200 == 0:\n",
    "                    evaluated = sess.run(g.get_train_ops(dataset_type='test'),\n",
    "                                        feed_dict=test_feed_dict)\n",
    "                    loss = g.extract_evaluated_tensors(evaluated, 'loss')\n",
    "                    g.add_summary(evaluated, step, dataset_type='test')\n",
    "                    \n",
    "                    buff = ''\n",
    "                    for l in loss:\n",
    "                        buff = buff + ' ,' + str(l)\n",
    "                    stdwrite(buff)\n",
    "            evaluated = sess.run(g.get_train_ops(dataset_type='validation'),\n",
    "                                feed_dict=valid_feed_dict)\n",
    "            g.add_summary(evaluated, step, dataset_type='validation')\n",
    "            g.update_noise_vals(evaluated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from simulation.simulation_builder import graph_builder as gb\n",
    "from importlib import reload\n",
    "from simulation.architectures import cifar10_architectures as c\n",
    "from simulation.read_datasets import Cifar10\n",
    "reload(gb)\n",
    "reload(c)\n",
    "g = gb.GraphBuilder(c.cnn_cifar10_architecture3,\n",
    "                   learning_rate=0.01,\n",
    "                   noise_list=[1.0, 0.9, 0.8, 0.7],\n",
    "                   noise_type='dropout',\n",
    "                   name='cnn_cifar10',\n",
    "                   summary_type='ordered_summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "n_epochs = 25\n",
    "with g._graph.as_default():\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    data_path = 'simulation/data/mnist/'\n",
    "    mnist = input_data.read_data_sets(data_path)\n",
    "    test_feed_dict = g.create_feed_dict(mnist.test.images,\n",
    "                                       mnist.test.labels,\n",
    "                                       dataset_type='test')\n",
    "    valid_feed_dict = g.create_feed_dict(mnist.validation.images,\n",
    "                                        mnist.validation.labels,\n",
    "                                        dataset_type='validation')\n",
    "    step = 0\n",
    "    n_iters = n_iters = mnist.train.num_examples // batch_size\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(g.variable_initializer)\n",
    "        for epoch in range(n_epochs):\n",
    "            for it in range(n_iters):\n",
    "                step += 1\n",
    "                X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "\n",
    "                evaluated = sess.run(g.get_train_ops(),\n",
    "                                    feed_dict=g.create_feed_dict(X_batch,\n",
    "                                                                y_batch))\n",
    "                g.add_summary(evaluated, step)\n",
    "                if it % 200 == 0:\n",
    "                    evaluated = sess.run(g.get_train_ops(dataset_type='test'),\n",
    "                                        feed_dict=test_feed_dict)\n",
    "                    loss = g.extract_evaluated_tensors(evaluated, 'loss')\n",
    "                    g.add_summary(evaluated, step, dataset_type='test')\n",
    "                    \n",
    "                    buff = ''\n",
    "                    for l in loss:\n",
    "                        buff = buff + ' ,' + str(l)\n",
    "                    stdwrite(buff)\n",
    "            evaluated = sess.run(g.get_train_ops(dataset_type='validation'),\n",
    "                                feed_dict=valid_feed_dict)\n",
    "            g.add_summary(evaluated, step, dataset_type='validation')\n",
    "            g.update_noise_vals(evaluated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar = r.Cifar10()\n",
    "X_data = cifar.train_data\n",
    "y_data = cifar.train_labels\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = islice((X_data, y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder_with_default(0.0, shape=[])\n",
    "y = 1.0 - x\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(tf.random_normal([2,2], stddev=1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(x))\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
